# The last variable MEDV is dependent/target variable, the remaining 13 vriables are dependent variables
summary(mydata)
? BostonHousing
# We can covert factor variable chas into numeric type
mydata$chas <- sapply(mydata$chas,as.numeric)   #sapply: Apply a Function over a List or Vector
str(mydata)  # ALL num NOW
################################
# Neural Network Model Building
# 4 layers: 1 input, 1 output, 2 hidden
NNmodel <- neuralnet(medv ~ crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+b+lstat, # 1 vs 13
data = mydata,     # we use all the mydata to build this model
hidden = c(10,5),  # two hidden layers, with 10 neuros in first while 5 neuros in second hidden layer
linear.output = F,
lifesign = 'full',
rep=1)
# Visualization
plot(NNmodel)
plot(NNmodel,
col.hidden = 'red',
col.hidden.synapse = 'darkgreen',
show.weights = F,
information = F,
fill = 'lightblue')  #node filling color
plot(NNmodel,
col.hidden = 'red',
col.hidden.synapse = 'darkgreen',
show.weights = F,
information = F,
fill = 'burlywood') #node filling color
################################################
# Let us seperate data into training and test
# Matrix
mydata <- as.matrix(mydata)    # turn data into matrix format, all our data are numeric
dimnames(mydata) <- NULL
# Partition
set.seed(1234)
labelindex <- sample(2, nrow(data), replace = T, prob = c(.7, .3))
training <- data[labelindex==1,1:13] #13 variables: Predictors/independent variables
test <- data[labelindex==2, 1:13]
labelindex <- sample(2, nrow(mydata), replace = T, prob = c(.7, .3))
training <- mydata[labelindex==1,1:13] #13 variables: Predictors/independent variables
test <- data[labelindex==2, 1:13]
labelindex <- sample(2, nrow(mydata), replace = T, prob = c(.7, .3))
training <- mydata[labelindex==1,1:13] #13 variables: Predictors/independent variables
test <- mydata[labelindex==2, 1:13]
trainingtarget <- mydata[labelindex==1, 14] #target variable: MEDV
testtarget <- mydata[labelindex==2, 14]
##########################################
# Normalize by (value-mean)/sd
# We should only use training data for normalization
# We can not use test that will leak information
Trainmean <- colMeans(training)  #use training data only. we get training mean for each variable
Trainsd <- apply(training, 2, sd) #use training data only. we get training sd for each variable
training <- scale(training, center = Trainmean, scale = Trainsd) #Normalize
test <- scale(test, center = Trainmean, scale = Trainsd)
# Create Model using Keras
model <- keras_model_sequential()
# Create Model using Keras
model <- keras_model_sequential()
library(keras)
install_keras()
# Create Model using Keras
model <- keras_model_sequential()
install_keras()
# Create Model using Keras
model <- keras_model_sequential()
model %>%  # layer_dense: fully connected, 1 hidden layer with 5 neuros, activation function relu, we have 13 input variables
layer_dense(units = 5, activation = 'relu', input_shape = c(13)) %>%
layer_dense(units = 1)  # 1 output layer
model %>%  # layer_dense: fully connected, 1 hidden layer with 5 neuros, activation function relu, we have 13 input variables
layer_dense(units = 5, activation = 'relu', input_shape = c(13)) %>%
layer_dense(units = 1)  # 1 output layer
# Compile
model %>% compile(loss = 'mse',  # we are doing regression problems (handle numeric prediction), mean square error
optimizer = 'rmsprop', # default optimizier
metrics = 'mae')  # Mean Absolute Error
# Fit Model
mymodel <- model %>%
fit(training,
trainingtarget,
epochs = 100,
batch_size = 32, #defines the number of samples to work through before updating the internal model parameters.
validation_split = 0.2)  # Using 20% data from training to evaluate error rate
# Evaluate
model %>% evaluate(test, testtarget)  # loss: mean square error
pred <- model %>% predict(test)
# square difference between targeted value in test data and predicted value - should be same with loss
mean((testtarget-pred)^2)
plot(testtarget, pred)  # if the data close to the diagonal,
model <- keras_model_sequential()
###################################
# 8.1. k-means clustering
irisDataForClustering <- iris
irisDataForClustering$Species <- NULL
kc <- kmeans(irisDataForClustering, 3)
kc
p <- ggplot(irisDataForClustering,
aes(x= Petal.Length, y=Petal.Width)) +
geom_point(color = kc$cluster, size=I(6)) +
theme(axis.text=element_text(size=16),
axis.title=element_text(size=16,face="bold"))
library(ggplot)
library(ggplot2)
p <- ggplot(irisDataForClustering,
aes(x= Petal.Length, y=Petal.Width)) +
geom_point(color = kc$cluster, size=I(6)) +
theme(axis.text=element_text(size=16),
axis.title=element_text(size=16,face="bold"))
p
##################################################################
# 8.2. Hierarchical Clustering
irisDataForClustering <- iris
irisDataForClustering$Species <- NULL
rnd = sample(dim(irisDataForClustering)[1])
# sample 40 data points for clustering for dispaly purpose
hc = hclust(dist(irisDataForClustering[rnd[1:40], ]))
plot(hc, hang = -1, labels = iris$Species[rnd[1:40]])
rect.hclust(hc, k=3)
groups = cutree(hc,3)
# Libraries
requiredPackages <- c("mice", "VIM")
if (length(setdiff(requiredPackages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(requiredPackages, rownames(installed.packages())))
}
library(mice)
library(VIM)
# Data vehicleMiss.csv
mydata <- read.csv(file.choose(), header = T)
mydata
# Compactly Display the Structure of an Arbitrary R data,
# so that we can have basic understanding
# 1. How many rows/objects
# 2. How many variables
# 3. Data type of each variable
str(mydata)
# We can understand more about our variables
# Note the difference between the numerica variables and categorical variables
# We can also understand whether each variable has missing value, i.e.
# number of "NA"
# We do not treat 0 as missing data
#NA's   :13      NA's   :6      NA's   :8      NA's   : 15
summary(mydata)
#########################################################
# Evalute how serious is the sitation of missing data?
#########################################################
# Compute percentage ourself by defining a function MissingPercentage
MissingPercentage <- function(datax) {sum(is.na(datax))/length(datax)*100}
apply(mydata, 2, MissingPercentage)
######################################
# using function md (Missing data pattern)
# Display missing-data patterns in matrix format
md.pattern(mydata)
###########################################
# Missing data patterns by variable pairs
md.pairs(mydata)
# Scatterplot with additional information in the margins
# In addition to a standard scatterplot, information about
# missing/imputed values is shown in the plot margins.
marginplot(data[,c('Mileage', 'lc')])
? marginplot
###########################################################
# Data Imputation using R's function mice
# mice: Multivariate Imputation by Chained Equations
##########################################################
# We can ignore the first column (vihicle ID: not useful) by using [2:7],
# which does not have preditive power
# Also does not have missing value
# m: Number of multiple imputations. The default is m=5.
# seed: An integer that is used as argument by the set.seed() for offsetting the random number generator. Default is to leave the random number generator alone.
impute <- mice(mydata[,2:7], m=3, seed = 123)
? mice
# We want to understand whatimputation method has been used for each variable
# that has missing values
print(impute)
# We want to take a look at the missing values
# each row tells us which row has missing value and what are
# the 3 predicted values
impute$imp$Mileage
#Let us take a look at the 19th row, in which Mileage=NA
mydata[19,]
#Let us take a look at the 20th row, in which Mileage=NA
mydata[20,]
#what is the average?
summary(mydata$Mileage)
#Let us take a look at the 253th row
mydata[253,]
# State =<NA>
impute$imp$State
mydata[68,]
######################################################################
# Complete data - here we have used the first imputation method "1"
newDATA1 <- complete(impute, 1)
newDATA1
#Let us take a look at the values that our three imputation methods
# have suggested
impute$imp$Mileage
###################################################################
# Complete data - here we have used the first imputation method "2"
newDATA2 <- complete(impute, 2)
newDATA2
# Blue dots are observed values
# Red dots are imputed values
# for fm: all its values are observed as it does not have
# missing values
# Mileage: 0: original data, 1: first imputation method
#          2: second imputation method
stripplot(impute, pch = 20, cex = 1.2)
# Scatter Plot for two variables
xyplot(impute, lc ~ lh | .imp, pch = 20, cex=1.4)
requiredPackages <- c("Boruta", "mlbench")
if (length(setdiff(requiredPackages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(requiredPackages, rownames(installed.packages())))
}
# Libraries
library(Boruta)  # Mythological God of forest, it based on random Forest
library(mlbench)
library(caret)
library(randomForest)
# Data Sonar
# discriminate between sonar signals bounced off a metal cylinder and
# those bounced off a roughly cylindrical rock.
data("Sonar")
str(Sonar)
##################################################
# Feature Selection with the Boruta algorithm
##################################################
set.seed(111)
boruta <- Boruta(Class ~ ., data = Sonar, doTrace = 2, maxRuns = 500)
print(boruta)
plot(boruta)
plot(boruta, las = 2, cex.axis = 0.7)
# Tentative Fix
Makedecision <- TentativeRoughFix(boruta)  # help us to make quick decision
print(Makedecision)
attStats(boruta)   # attribute statistics
# Data Partition
set.seed(222)
ind <- sample(2, nrow(Sonar), replace = T, prob = c(0.6, 0.4))
train <- Sonar[ind==1,]
test <- Sonar[ind==2,]
###########################
# Random Forest Model
set.seed(333)
rf60 <- randomForest(Class~., data = train)
rf60
# Prediction & Confusion Matrix - Test
pretest1 <- predict(rf60, test)
pretest1
confusionMatrix(pretest1, test$Class)
#################################################
# Let us use confirmed + tentative
# We get all those important (33) and tentative (8) features
getNonRejectedFormula(boruta)
#Let us copy the result which include 41 features
#also let us use the same seed
#set.seed(333)
rf41 <- randomForest(Class ~ V1 + V2 + V4 + V5 + V8 + V9 + V10 + V11 + V12 + V13 +
V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 +
V26 + V27 + V28 + V30 + V31 + V32 + V35 + V36 + V37 + V39 +
V43 + V44 + V45 + V46 + V47 + V48 + V49 + V51 + V52 + V54,
data = train)
rf41
# Prediction & Confusion Matrix - Test
pretest2 <- predict(rf41, test)
pretest2
confusionMatrix(pretest2, test$Class)
#################################################
# Let us use Confirmed ONLY
getConfirmedFormula(boruta)
#Let us copy the result which include 41 features
#also let us use the same seed
#set.seed(333)
rf33 <- randomForest(Class ~ V1 + V4 + V5 + V9 + V10 + V11 + V12 + V13 + V15 + V16 +
V17 + V18 + V19 + V20 + V21 + V22 + V23 + V26 + V27 + V28 +
V31 + V35 + V36 + V37 + V43 + V44 + V45 + V46 + V47 + V48 +
V49 + V51 + V52,
data = train)
rf33
# Prediction & Confusion Matrix - Test
pretest3 <- predict(rf33, test)
pretest3
confusionMatrix(pretest3, test$Class)
# Note we have largely reduce the number of features and
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# machine learning models, such as neural network and deep learning
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# machine learning models, such as neural network and deep learning
# that requires long time to learn
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# machine learning models, such as neural network and deep learning
# that requires long time to learn
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# machine learning models, such as neural network and deep learning
# that requires long time to learn
# Libraries
requiredPackages <- c("mice", "VIM")
if (length(setdiff(requiredPackages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(requiredPackages, rownames(installed.packages())))
}
library(mice)
library(VIM)
# Data vehicleMiss.csv
mydata <- read.csv(file.choose(), header = T)
# Load packages
library('ggplot2')
library('ggthemes')
library('scales')
library('dplyr')
library('mice')
library('randomForest')
library('plyr')
#install.packages("corrplot")
library(corrplot)
setwd("~/Desktop/MSA/msa_mh8111_as1_kaggle_project/")
Train <- read.csv("train.csv", header=TRUE,stringsAsFactors = FALSE, na.strings=c("", "NA"))
Test <- read.csv("test.csv", header=TRUE,stringsAsFactors = FALSE, na.strings=c("", "NA"))
#raw data exploration for missing data
summary(is.na(Train))
summary(is.na(Test))
myfunction <- function(my_data)
{
feature_names = colnames(my_data)
for (col_i in feature_names)
{
is_na = is.na(my_data[col_i])
if (sum(is_na))
{
print(summary(is_na))
}
}
}
myfunction(Train)
myfunction(Test)
#new feature Per Passenger Fare
Train$PerPassengerFare = Train$Fare / (Train$SibSp + Train$Parch + 1)
Train <- subset(Train, select = -c(Fare))
summary(Train)
#new feature NoFare
print(subset(Test, Test$Fare == 0))
print(subset(Test, Test$Ticket == "LINE"))
print(subset(Train, Train$PerPassengerFare == 0))
print(subset(Train, Train$Ticket == "LINE"))
Train$NoFare <- (Train$PerPassengerFare < 0.01)
print(sum(Train$NoFare == TRUE))
summary(Train)
#new feature FamilySize
Train$FamilySize = Train$SibSp + Train$Parch + 1
Train <- subset(Train, select = -c(SibSp, Parch))
summary(Train)
#new feature Social Status by Ming Xiu [TODO: to copy over]
Train$Title <- gsub('(.*, )|(\\..*)', '', Train$Name)
commoner_title <- c('Dona', 'Don', 'Miss','Mlle','Mme','Mr','Mrs','Ms')
royalty_title <- c('Lady', 'the Countess', 'Sir', 'Jonkheer', 'Master')
rank_title <- c('Capt', 'Col', 'Dr', 'Major', 'Rev')
Train$SocialClass[Train$Title %in% commoner_title] <- 'commoner'
Train$SocialClass[Train$Title %in% royalty_title] <- 'royal'
Train$SocialClass[Train$Title %in% rank_title] <- 'rank'
Train <- subset(Train, select = -c(Cabin))
summary(Train)
#Per Passenger Fare grouped by Pclass
p1AvgFare = mean(subset(Train$PerPassengerFare, Train$Pclass == 1))
p1AvgFare
p2AvgFare = mean(subset(Train$PerPassengerFare, Train$Pclass == 2))
p2AvgFare
p3AvgFare = mean(subset(Train$PerPassengerFare, Train$Pclass == 3))
p3AvgFare
#Handling missing data Fare in Test
#filling the missing Fare in Test, [TODO] need compute avgFare by class combining both Train and Test later
print(subset(Test, is.na(Test$Fare)))
#Handling missing data in Embarked with the most frequent value
##argument, only 2 are missing, there is no great value digging into it.
tableEmbarkedReq = count(Train$Embarked)
mostFreqValue = tableEmbarkedReq$x[which(tableEmbarkedReq$freq == max(tableEmbarkedReq$freq))]
Train$Embarked[is.na(Train$Embarked)] <- mostFreqValue
summary(is.na(Train))
#partial optimization
Train$Embarked <- as.factor(Train$Embarked)
Train$Sex <- as.factor(Train$Sex)
Train$Pclass <- as.factor(Train$Pclass)
Train$Title <- as.factor(Train$Title)
Train$SocialClass <- as.factor(Train$SocialClass)
summary(is.na(Train))
#Handling missing data in Age Approach 1:
init = mice(Train, maxit=0)
predM = init$predictorMatrix
#remove PassengerId, Ticket column from the predicate
predM[, c("PassengerId", "Ticket")]=0
imp<-mice(Train, m=5, predictorMatrix = predM)
Train1 <- complete(imp)
summary(Train1)
summary(is.na(Train1))
summary(Train1)
dim(Train1)
#non-intuitive feature important
# 1. feature important using random forest
set.seed(100)
rf_model <- randomForest(factor(Survived) ~ Pclass + Sex + Age + FamilySize +
PerPassengerFare + Embarked + NoFare + Title + SocialClass,
data = Train1,
importance = TRUE)
importance(rf_model)
featureImportance <- varImpPlot(rf_model, sort = T, n.var = 9, main = "Top 9 - Variable Importance")
plot(featureImportance)
rf_model$confusion
# load the library
library(mlbench)
library(caret)
Train1_2 <- Train1[, c("Survived", "Pclass","Sex","Age","FamilySize","PerPassengerFare", "NoFare", "Title", "SocialClass")]
# prepare training scheme
#control <- trainControl(method="repeatedcv", number=10, repeats=3)
control <- trainControl(method="cv", number=5)
# train the model
model <- train(Survived~., data=Train1_2, trControl=control, importance=TRUE)
featureImportance <- varImp(model)
print(featureImportance)
#Handling missing data in Age Approach 2:
#linear regression Age as function of most correlated features
summary(is.na(Train))
#Handling missing data in Age Approach 2:
#linear regression Age as function of most correlated features
summary(is.na(Train))
Train2_1 <-Train[is.na(Train$Age) == false, ]
Train2_1 <-Train[is.na(Train$Age) = false, ]
Train2_1 <-Train[is.na(Train$Age), ]
summary(is.na(Train2_1))
AgeTrainToPredict <-Train[is.na(Train$Age), ]
summary(is.na(AgeTrainToPredict))
AgeTrainLRSet <- Train[!is.na(Train$Age), ]
summary(is.na(AgeTrainLRSet))
AgeTrainLRSet <- Train[!is.na(Train$Age), ]
summary(is.na(AgeTrainLRSet))
AgeTrain_LR <- AgeTrainLRSet[, c("Pclass","Sex","Age","FamilySize","PerPassengerFare","Embarked", "NoFare", "Title", "SocialClass")]
AgeTrain_LR <- AgeTrain_LR %>% mutate_if(is.factor, as.numeric)
corVal <- round(cor(AgeTrain_LR, method = c("pearson")), 2)
corVal
corVal <- round(abs(cor(AgeTrain_LR, method = c("pearson"))), 2)
corVal
corVal$Age
corVal[,]
corVal["Age",]
sort(corVal["Age",])
sort(corVal["Age",])[1:5]
sort(corVal["Age",], decreasing = TRUE)[2:6]
LR_x_set
LR_x_set <- sort(corVal["Age",], decreasing = TRUE)[2:6]
LR_x_set
AgeTrain_LR_selected_X <- AgeTrain_LR(, LR_x_set)
AgeTrain_LR <- AgeTrainLRSet[, c("Pclass","Sex","Age","FamilySize","PerPassengerFare","Embarked", "NoFare", "Title", "SocialClass")]
AgeTrain_LR <- AgeTrain_LR %>% mutate_if(is.factor, as.numeric)
corVal <- round(abs(cor(AgeTrain_LR, method = c("pearson"))), 2)
#find the top 5 features correlated with Age in the non-NA set
LR_x_set <- sort(corVal["Age",], decreasing = TRUE)[2:6]
AgeTrain_LR_selected_X <- AgeTrain_LR(, LR_x_set)
AgeTrain_LR_selected_X <- AgeTrain_LR[, LR_x_set]
ageFit <- lm(LR_y_set ~ LR_x_set[[1]], data = AgeTrain_LR_selected_X)
AgeTrain_LR_selected_X <- AgeTrain_LR[, LR_x_set]
ageFit <- lm(LR_y_set ~ LR_x_set[1:5], data = AgeTrain_LR_selected_X)
ageFit <- lm(LR_y_set$Age ~ LR_x_set[1:5], data = AgeTrain_LR_selected_X)
ageFit <- lm(AgeTrain_LR_selected_X$Age ~ LR_x_set[1:5], data = AgeTrain_LR_selected_X)
ageFit <- lm(AgeTrain_LR$Age ~ LR_x_set[1:5], data = AgeTrain_LR_selected_X)
ageFit <- lm(AgeTrain_LR$Age ~ LR_x_set[1], data = AgeTrain_LR_selected_X)
ageFit <- lm(Age ~ LR_x_set[1], data = AgeTrain_LR_selected_X)
ageFit <- lm(Age ~ LR_x_set[1], data = AgeTrain_LR)
ageFit <- lm(Age ~ LR_x_set[1:5], data = AgeTrain_LR)
print(LR_x_set)
ageFit <- lm(Age ~ Pclass + Title + FamilySize + SocialClass + PerPassengerFare, data = AgeTrain_LR)
summary(fit) # show results
summary(ageFit)
coeffs = cofficient(ageFit)
coeffs = coefficients(ageFit)
coeffs
#find the top 5 features correlated with Age in the non-NA set
top_5_cor_features <- sort(corVal["Age",], decreasing = TRUE)[2:6]
print(top_5_cor_features)
summary(ageFit)
summary(predict(ageFit, AgeTrainToPredict))
predict(ageFit, AgeTrainToPredict)
#finding a multi linear regression model with the five features
ageFit <- lm(Age ~ Pclass + Title + FamilySize + SocialClass + PerPassengerFare, data = AgeTrainLRSet)
coeffs = coefficients(ageFit)
print(coeffs)
#check out this page for the metric explaination
#http://r-statistics.co/Linear-Regression.html
summary(ageFit)
predict(ageFit, AgeTrainToPredict)
summary(predict(ageFit, AgeTrainToPredict))
predict(ageFit, AgeTrainToPredict)
pred <- predict(ageFit, AgeTrainToPredict)
summary(pred)
(count(pred))
(dim(pred))
dim(pred)
pred <- predict(ageFit, AgeTrainToPredict)
pred <- predict(ageFit, AgeTrainToPredict)
#Handling missing data in Age Approach 2:
#linear regression Age as function of most correlated features
summary(is.na(Train))
dim(pred)
(pred)
summary(pred)
pred[1]
pred[1][2]
pred[,]
dim(predRes)
predRes <- predict(ageFit, AgeTrainToPredict)
dim(predRes)
