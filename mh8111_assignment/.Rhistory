dim(iris)
names(iris)
#Plot Petal.Length vs. Petal.Width:
plot (iris[ , 3], iris[ , 4]);
example(plot)
#Plot Petal.Length vs. Petal.Width:
plot (iris[ , 3], iris[ , 4]);
example(plot)
objects()
ls()
getwd()
??read.csv2
?read.csv2
library(xlsx)
install.packages(xlsx)
install.packages('xlsx')
library(xlsx)
#You can also use R's built in spreadsheet to enter the data
#interactively, as in the following example.
# enter data using editor
mydata <- data.frame(age=numeric(0), gender=character(0), weight=numeric(0))
mydata <- edit(mydata) # note that without the assignment in the line above,
#You can also use R's built in spreadsheet to enter the data
#interactively, as in the following example.
# enter data using editor
mydata <- data.frame(age=numeric(0), gender=character(0), weight=numeric(0))
mydata <- edit(mydata) # note that without the assignment in the line above,
install.packages('X11')
requiredPackages <- c("keras", "mlbench", "dplyr", "magrittr", "neuralnet")
if (length(setdiff(requiredPackages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(requiredPackages, rownames(installed.packages())))
}
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
# Data
data("BostonHousing")  # This data is in mlbench package
mydata <- BostonHousing  # We rename it simply as mydata
str(mydata)  #Can take a look at the #obs. and variables, Can click the BostonHousing under Global Enviroment
# The last variable MEDV is dependent/target variable, the remaining 13 vriables are dependent variables
summary(mydata)
? BostonHousing
# We can covert factor variable chas into numeric type
mydata$chas <- sapply(mydata$chas,as.numeric)   #sapply: Apply a Function over a List or Vector
str(mydata)  # ALL num NOW
################################
# Neural Network Model Building
# 4 layers: 1 input, 1 output, 2 hidden
NNmodel <- neuralnet(medv ~ crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+b+lstat, # 1 vs 13
data = mydata,     # we use all the mydata to build this model
hidden = c(10,5),  # two hidden layers, with 10 neuros in first while 5 neuros in second hidden layer
linear.output = F,
lifesign = 'full',
rep=1)
# Visualization
plot(NNmodel)
plot(NNmodel,
col.hidden = 'red',
col.hidden.synapse = 'darkgreen',
show.weights = F,
information = F,
fill = 'lightblue')  #node filling color
plot(NNmodel,
col.hidden = 'red',
col.hidden.synapse = 'darkgreen',
show.weights = F,
information = F,
fill = 'burlywood') #node filling color
################################################
# Let us seperate data into training and test
# Matrix
mydata <- as.matrix(mydata)    # turn data into matrix format, all our data are numeric
dimnames(mydata) <- NULL
# Partition
set.seed(1234)
labelindex <- sample(2, nrow(data), replace = T, prob = c(.7, .3))
training <- data[labelindex==1,1:13] #13 variables: Predictors/independent variables
test <- data[labelindex==2, 1:13]
labelindex <- sample(2, nrow(mydata), replace = T, prob = c(.7, .3))
training <- mydata[labelindex==1,1:13] #13 variables: Predictors/independent variables
test <- data[labelindex==2, 1:13]
labelindex <- sample(2, nrow(mydata), replace = T, prob = c(.7, .3))
training <- mydata[labelindex==1,1:13] #13 variables: Predictors/independent variables
test <- mydata[labelindex==2, 1:13]
trainingtarget <- mydata[labelindex==1, 14] #target variable: MEDV
testtarget <- mydata[labelindex==2, 14]
##########################################
# Normalize by (value-mean)/sd
# We should only use training data for normalization
# We can not use test that will leak information
Trainmean <- colMeans(training)  #use training data only. we get training mean for each variable
Trainsd <- apply(training, 2, sd) #use training data only. we get training sd for each variable
training <- scale(training, center = Trainmean, scale = Trainsd) #Normalize
test <- scale(test, center = Trainmean, scale = Trainsd)
# Create Model using Keras
model <- keras_model_sequential()
# Create Model using Keras
model <- keras_model_sequential()
library(keras)
install_keras()
# Create Model using Keras
model <- keras_model_sequential()
install_keras()
# Create Model using Keras
model <- keras_model_sequential()
model %>%  # layer_dense: fully connected, 1 hidden layer with 5 neuros, activation function relu, we have 13 input variables
layer_dense(units = 5, activation = 'relu', input_shape = c(13)) %>%
layer_dense(units = 1)  # 1 output layer
model %>%  # layer_dense: fully connected, 1 hidden layer with 5 neuros, activation function relu, we have 13 input variables
layer_dense(units = 5, activation = 'relu', input_shape = c(13)) %>%
layer_dense(units = 1)  # 1 output layer
# Compile
model %>% compile(loss = 'mse',  # we are doing regression problems (handle numeric prediction), mean square error
optimizer = 'rmsprop', # default optimizier
metrics = 'mae')  # Mean Absolute Error
# Fit Model
mymodel <- model %>%
fit(training,
trainingtarget,
epochs = 100,
batch_size = 32, #defines the number of samples to work through before updating the internal model parameters.
validation_split = 0.2)  # Using 20% data from training to evaluate error rate
# Evaluate
model %>% evaluate(test, testtarget)  # loss: mean square error
pred <- model %>% predict(test)
# square difference between targeted value in test data and predicted value - should be same with loss
mean((testtarget-pred)^2)
plot(testtarget, pred)  # if the data close to the diagonal,
model <- keras_model_sequential()
###################################
# 8.1. k-means clustering
irisDataForClustering <- iris
irisDataForClustering$Species <- NULL
kc <- kmeans(irisDataForClustering, 3)
kc
p <- ggplot(irisDataForClustering,
aes(x= Petal.Length, y=Petal.Width)) +
geom_point(color = kc$cluster, size=I(6)) +
theme(axis.text=element_text(size=16),
axis.title=element_text(size=16,face="bold"))
library(ggplot)
library(ggplot2)
p <- ggplot(irisDataForClustering,
aes(x= Petal.Length, y=Petal.Width)) +
geom_point(color = kc$cluster, size=I(6)) +
theme(axis.text=element_text(size=16),
axis.title=element_text(size=16,face="bold"))
p
##################################################################
# 8.2. Hierarchical Clustering
irisDataForClustering <- iris
irisDataForClustering$Species <- NULL
rnd = sample(dim(irisDataForClustering)[1])
# sample 40 data points for clustering for dispaly purpose
hc = hclust(dist(irisDataForClustering[rnd[1:40], ]))
plot(hc, hang = -1, labels = iris$Species[rnd[1:40]])
rect.hclust(hc, k=3)
groups = cutree(hc,3)
# Libraries
requiredPackages <- c("mice", "VIM")
if (length(setdiff(requiredPackages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(requiredPackages, rownames(installed.packages())))
}
library(mice)
library(VIM)
# Data vehicleMiss.csv
mydata <- read.csv(file.choose(), header = T)
mydata
# Compactly Display the Structure of an Arbitrary R data,
# so that we can have basic understanding
# 1. How many rows/objects
# 2. How many variables
# 3. Data type of each variable
str(mydata)
# We can understand more about our variables
# Note the difference between the numerica variables and categorical variables
# We can also understand whether each variable has missing value, i.e.
# number of "NA"
# We do not treat 0 as missing data
#NA's   :13      NA's   :6      NA's   :8      NA's   : 15
summary(mydata)
#########################################################
# Evalute how serious is the sitation of missing data?
#########################################################
# Compute percentage ourself by defining a function MissingPercentage
MissingPercentage <- function(datax) {sum(is.na(datax))/length(datax)*100}
apply(mydata, 2, MissingPercentage)
######################################
# using function md (Missing data pattern)
# Display missing-data patterns in matrix format
md.pattern(mydata)
###########################################
# Missing data patterns by variable pairs
md.pairs(mydata)
# Scatterplot with additional information in the margins
# In addition to a standard scatterplot, information about
# missing/imputed values is shown in the plot margins.
marginplot(data[,c('Mileage', 'lc')])
? marginplot
###########################################################
# Data Imputation using R's function mice
# mice: Multivariate Imputation by Chained Equations
##########################################################
# We can ignore the first column (vihicle ID: not useful) by using [2:7],
# which does not have preditive power
# Also does not have missing value
# m: Number of multiple imputations. The default is m=5.
# seed: An integer that is used as argument by the set.seed() for offsetting the random number generator. Default is to leave the random number generator alone.
impute <- mice(mydata[,2:7], m=3, seed = 123)
? mice
# We want to understand whatimputation method has been used for each variable
# that has missing values
print(impute)
# We want to take a look at the missing values
# each row tells us which row has missing value and what are
# the 3 predicted values
impute$imp$Mileage
#Let us take a look at the 19th row, in which Mileage=NA
mydata[19,]
#Let us take a look at the 20th row, in which Mileage=NA
mydata[20,]
#what is the average?
summary(mydata$Mileage)
#Let us take a look at the 253th row
mydata[253,]
# State =<NA>
impute$imp$State
mydata[68,]
######################################################################
# Complete data - here we have used the first imputation method "1"
newDATA1 <- complete(impute, 1)
newDATA1
#Let us take a look at the values that our three imputation methods
# have suggested
impute$imp$Mileage
###################################################################
# Complete data - here we have used the first imputation method "2"
newDATA2 <- complete(impute, 2)
newDATA2
# Blue dots are observed values
# Red dots are imputed values
# for fm: all its values are observed as it does not have
# missing values
# Mileage: 0: original data, 1: first imputation method
#          2: second imputation method
stripplot(impute, pch = 20, cex = 1.2)
# Scatter Plot for two variables
xyplot(impute, lc ~ lh | .imp, pch = 20, cex=1.4)
requiredPackages <- c("Boruta", "mlbench")
if (length(setdiff(requiredPackages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(requiredPackages, rownames(installed.packages())))
}
# Libraries
library(Boruta)  # Mythological God of forest, it based on random Forest
library(mlbench)
library(caret)
library(randomForest)
# Data Sonar
# discriminate between sonar signals bounced off a metal cylinder and
# those bounced off a roughly cylindrical rock.
data("Sonar")
str(Sonar)
##################################################
# Feature Selection with the Boruta algorithm
##################################################
set.seed(111)
boruta <- Boruta(Class ~ ., data = Sonar, doTrace = 2, maxRuns = 500)
print(boruta)
plot(boruta)
plot(boruta, las = 2, cex.axis = 0.7)
# Tentative Fix
Makedecision <- TentativeRoughFix(boruta)  # help us to make quick decision
print(Makedecision)
attStats(boruta)   # attribute statistics
# Data Partition
set.seed(222)
ind <- sample(2, nrow(Sonar), replace = T, prob = c(0.6, 0.4))
train <- Sonar[ind==1,]
test <- Sonar[ind==2,]
###########################
# Random Forest Model
set.seed(333)
rf60 <- randomForest(Class~., data = train)
rf60
# Prediction & Confusion Matrix - Test
pretest1 <- predict(rf60, test)
pretest1
confusionMatrix(pretest1, test$Class)
#################################################
# Let us use confirmed + tentative
# We get all those important (33) and tentative (8) features
getNonRejectedFormula(boruta)
#Let us copy the result which include 41 features
#also let us use the same seed
#set.seed(333)
rf41 <- randomForest(Class ~ V1 + V2 + V4 + V5 + V8 + V9 + V10 + V11 + V12 + V13 +
V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 +
V26 + V27 + V28 + V30 + V31 + V32 + V35 + V36 + V37 + V39 +
V43 + V44 + V45 + V46 + V47 + V48 + V49 + V51 + V52 + V54,
data = train)
rf41
# Prediction & Confusion Matrix - Test
pretest2 <- predict(rf41, test)
pretest2
confusionMatrix(pretest2, test$Class)
#################################################
# Let us use Confirmed ONLY
getConfirmedFormula(boruta)
#Let us copy the result which include 41 features
#also let us use the same seed
#set.seed(333)
rf33 <- randomForest(Class ~ V1 + V4 + V5 + V9 + V10 + V11 + V12 + V13 + V15 + V16 +
V17 + V18 + V19 + V20 + V21 + V22 + V23 + V26 + V27 + V28 +
V31 + V35 + V36 + V37 + V43 + V44 + V45 + V46 + V47 + V48 +
V49 + V51 + V52,
data = train)
rf33
# Prediction & Confusion Matrix - Test
pretest3 <- predict(rf33, test)
pretest3
confusionMatrix(pretest3, test$Class)
# Note we have largely reduce the number of features and
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# machine learning models, such as neural network and deep learning
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# machine learning models, such as neural network and deep learning
# that requires long time to learn
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# machine learning models, such as neural network and deep learning
# that requires long time to learn
# Note we have largely reduce the number of features and
# achieved simiar accuracies. This is particularly useful for those
# machine learning models, such as neural network and deep learning
# that requires long time to learn
# Libraries
requiredPackages <- c("mice", "VIM")
if (length(setdiff(requiredPackages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(requiredPackages, rownames(installed.packages())))
}
library(mice)
library(VIM)
# Data vehicleMiss.csv
mydata <- read.csv(file.choose(), header = T)
X <- c(3, 8, 10, 11, 13, 16, 27, 30, 35, 37, 38, 44, 103, 142)
Y <- c(4, 7, 8, 8, 10, 11, 16, 26, 21, 9, 31, 30, 75, 90)
dim(x)
dim(X)
X
summary(X)
summary(Y)
xy <- rbind(X, Y)
xy
lm(Y ~ X, data=xy)
xy <- as.data.frame(xy)
lm(Y ~ X, data=xy)
model <- lm(Y ~ X, data=xy)
summary(model)
setwd("~/Desktop/MSA/msa_mh8101_or1_lp/mh8111_assignment")
# read the sms data into the sms data frame
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)
# examine the structure of the sms data
str(sms_raw)
# convert spam/ham to factor.
sms_raw$type <- factor(sms_raw$type)
# examine the type variable more carefully
str(sms_raw$type)
table(sms_raw$type)
# build a corpus using the text mining (tm) package
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
install.packages(c("gmodels", "SnowballC", "tm", "wordcloud"))
# convert spam/ham to factor.
sms_raw$type <- factor(sms_raw$type)
# examine the type variable more carefully
str(sms_raw$type)
table(sms_raw$type)
# build a corpus using the text mining (tm) package
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
# examine the sms corpus
print(sms_corpus)
inspect(sms_corpus[1:2])
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:2], as.character)
# clean up the corpus using tm_map()
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# show the difference between sms_corpus and corpus_clean
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers) # remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords()) # remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation) # remove punctuation
# tip: create a custom function to replace (rather than remove) punctuation
removePunctuation("hello...world")
replacePunctuation <- function(x) { gsub("[[:punct:]]+", " ", x) }
replacePunctuation("hello...world")
# illustration of word stemming
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace) # eliminate unneeded whitespace
# examine the final clean corpus
lapply(sms_corpus[1:3], as.character)
lapply(sms_corpus_clean[1:3], as.character)
# create a document-term sparse matrix
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
# alternative solution: create a document-term sparse matrix directly from the SMS corpus
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
tolower = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
# alternative solution: using custom stop words function ensures identical result
sms_dtm3 <- DocumentTermMatrix(sms_corpus, control = list(
tolower = TRUE,
removeNumbers = TRUE,
stopwords = function(x) { removeWords(x, stopwords()) },
removePunctuation = TRUE,
stemming = TRUE
))
# compare the result
sms_dtm
sms_dtm2
sms_dtm3
# creating training and test datasets
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]
# also save the labels
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels  <- sms_raw[4170:5559, ]$type
# check that the proportion of spam is similar
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
# word cloud visualization
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
# subset the training data into spam and ham groups
spam <- subset(sms_raw, type == "spam")
# check that the proportion of spam is similar
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
# word cloud visualization
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
# subset the training data into spam and ham groups
spam <- subset(sms_raw, type == "spam")
ham  <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
sms_dtm_freq_train <- removeSparseTerms(sms_dtm_train, 0.999)
sms_dtm_freq_train
# indicator features for frequent words
findFreqTerms(sms_dtm_train, 5)
# save frequently-appearing terms to a character vector
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
# create DTMs with only the frequent terms
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
# build a corpus using the text mining (tm) package
#library(tm)
#sms_corpus <- VCorpus(VectorSource(sms_raw$text))
sms_corpus <- Corpus(VectorSource(sms_raw$text))
# examine the sms corpus
print(sms_corpus)
inspect(sms_corpus[1:2])
# examine the sms corpus
print(sms_corpus)
inspect(sms_corpus[1:2])
setwd("~/Desktop/MSA/msa_mh8101_or1_lp/mh8111_assignment")
# build a corpus using the text mining (tm) package
#library(tm)
#sms_corpus <- VCorpus(VectorSource(sms_raw$text))
sms_corpus <- Corpus(VectorSource(sms_raw$text))
# examine the sms corpus
print(sms_corpus)
inspect(sms_corpus[1:2])
# read the sms data into the sms data frame
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)
# examine the structure of the sms data
str(sms_raw)
# convert spam/ham to factor.
sms_raw$type <- factor(sms_raw$type)
# examine the type variable more carefully
str(sms_raw$type)
table(sms_raw$type)
# build a corpus using the text mining (tm) package
#library(tm)
#sms_corpus <- VCorpus(VectorSource(sms_raw$text))
sms_corpus <- Corpus(VectorSource(sms_raw$text))
# examine the sms corpus
print(sms_corpus)
inspect(sms_corpus[1:2])
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:2], as.character)
# examine the type variable more carefully
str(sms_raw$type)
table(sms_raw$type)
# build a corpus using the text mining (tm) package
#library(tm)
#sms_corpus <- VCorpus(VectorSource(sms_raw$text))
sms_corpus <- Corpus(VectorSource(sms_raw$text))
# examine the sms corpus
print(sms_corpus)
